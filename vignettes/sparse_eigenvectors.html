<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Konstantinos Benidis and Daniel P. Palomar" />

<meta name="date" content="2017-11-25" />

<title>Computing sparse eigenvectors</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Computing sparse eigenvectors</h1>
<h4 class="author"><em>Konstantinos Benidis and Daniel P. Palomar</em></h4>
<h4 class="date"><em>2017-11-25</em></h4>


<div id="TOC">
<ul>
<li><a href="#comparison-with-other-packages"><span class="toc-section-number">1</span> Comparison with other packages</a></li>
<li><a href="#usage-of-the-package"><span class="toc-section-number">2</span> Usage of the package</a><ul>
<li><a href="#computation-of-sparse-eigenvectors-of-a-given-matrix"><span class="toc-section-number">2.1</span> Computation of sparse eigenvectors of a given matrix</a></li>
<li><a href="#covariance-matrix-estimation-with-sparse-eigenvectors"><span class="toc-section-number">2.2</span> Covariance matrix estimation with sparse eigenvectors</a></li>
<li><a href="#complex-valued-inputs"><span class="toc-section-number">2.3</span> Complex-valued inputs</a></li>
</ul></li>
<li><a href="#explanation-of-the-algorithms"><span class="toc-section-number">3</span> Explanation of the algorithms</a><ul>
<li><a href="#speigen-sparse-eigenvectors-from-a-given-covariance-matrix"><span class="toc-section-number">3.1</span> <code>spEigen()</code>: Sparse eigenvectors from a given covariance matrix</a></li>
<li><a href="#speigencov-covariance-matrix-estimation-with-sparse-eigenvectors"><span class="toc-section-number">3.2</span> <code>spEigenCov()</code>: Covariance matrix estimation with sparse eigenvectors</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<hr />
<p>This vignette illustrates the computation of sparse eigenvectors or sparse PCA with the package <code>sparseEigen</code> (with a comparison with other packages) and gives a description of the algorithms used.</p>
<div id="comparison-with-other-packages" class="section level1">
<h1><span class="header-section-number">1</span> Comparison with other packages</h1>
<p>hulsdhiuvhsdal</p>
</div>
<div id="usage-of-the-package" class="section level1">
<h1><span class="header-section-number">2</span> Usage of the package</h1>
<div id="computation-of-sparse-eigenvectors-of-a-given-matrix" class="section level2">
<h2><span class="header-section-number">2.1</span> Computation of sparse eigenvectors of a given matrix</h2>
<p>We start by loading the package and generating synthetic data with sparse eigenvectors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparseEigen)
<span class="kw">set.seed</span>(<span class="dv">42</span>)

<span class="co"># parameters </span>
m &lt;-<span class="st"> </span><span class="dv">500</span>  <span class="co"># dimension</span>
n &lt;-<span class="st"> </span><span class="dv">100</span>  <span class="co"># number of samples</span>
q &lt;-<span class="st"> </span><span class="dv">3</span>  <span class="co"># number of sparse eigenvectors to be estimated</span>
sp_card &lt;-<span class="st"> </span><span class="fl">0.2</span><span class="op">*</span>m  <span class="co"># cardinality of each sparse eigenvector</span>
rho &lt;-<span class="st"> </span><span class="fl">0.6</span>  <span class="co"># sparsity level</span>

<span class="co"># generate non-overlapping sparse eigenvectors</span>
V &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, m, q)
V[<span class="kw">cbind</span>(<span class="kw">seq</span>(<span class="dv">1</span>, q<span class="op">*</span>sp_card), <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>q, <span class="dt">each =</span> sp_card))] &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(sp_card)
V &lt;-<span class="st"> </span><span class="kw">cbind</span>(V, <span class="kw">matrix</span>(<span class="kw">rnorm</span>(m<span class="op">*</span>(m<span class="op">-</span>q)), m, m<span class="op">-</span>q))
<span class="co"># keep first q eigenvectors the same (already orthogonal) and orthogonalize the rest</span>
V &lt;-<span class="st"> </span><span class="kw">qr.Q</span>(<span class="kw">qr</span>(V))  

<span class="co"># generate eigenvalues</span>
lmd &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span><span class="op">*</span><span class="kw">seq</span>(<span class="dt">from =</span> q, <span class="dt">to =</span> <span class="dv">1</span>), <span class="kw">rep</span>(<span class="dv">1</span>, m<span class="op">-</span>q))

<span class="co"># generate covariance matrix from sparse eigenvectors and eigenvalues</span>
R &lt;-<span class="st"> </span>V <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(lmd) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(V)

<span class="co"># generate data matrix from a zero-mean multivariate Gaussian distribution </span>
<span class="co"># with the constructed covariance</span>
X &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(n, <span class="kw">rep</span>(<span class="dv">0</span>, m), R)  <span class="co"># random data with underlying sparse structure</span></code></pre></div>
<p>Then, we estimate the covariance matrix with <code>cov(X)</code> and compute its sparse eigenvectors with <code>spEigen()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># computation of sparse eigenvectors</span>
res_standard &lt;-<span class="st"> </span><span class="kw">eigen</span>(<span class="kw">cov</span>(X))
res_sparse1 &lt;-<span class="st"> </span><span class="kw">spEigen</span>(<span class="kw">cov</span>(X), q, rho)
res_sparse2 &lt;-<span class="st"> </span><span class="kw">spEigen</span>(X, q, rho, <span class="dt">data =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>We can assess how good the estimated eigenvectors are by computing the inner product with the original eigenvectors (the closer to 1 the better):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show inner product between estimated eigenvectors and originals</span>
<span class="kw">abs</span>(<span class="kw">diag</span>(<span class="kw">t</span>(res_standard<span class="op">$</span>vectors) <span class="op">%*%</span><span class="st"> </span>V[, <span class="dv">1</span><span class="op">:</span>q]))   <span class="co">#for standard estimated eigenvectors</span>
<span class="co">#&gt; [1] 0.9215392 0.9194898 0.9740871</span>
<span class="kw">abs</span>(<span class="kw">diag</span>(<span class="kw">t</span>(res_sparse1<span class="op">$</span>vectors) <span class="op">%*%</span><span class="st"> </span>V[, <span class="dv">1</span><span class="op">:</span>q]))    <span class="co">#for sparse estimated eigenvectors</span>
<span class="co">#&gt; [1] 0.9971061 0.9969231 0.9922915</span>
<span class="kw">abs</span>(<span class="kw">diag</span>(<span class="kw">t</span>(res_sparse2<span class="op">$</span>vectors) <span class="op">%*%</span><span class="st"> </span>V[, <span class="dv">1</span><span class="op">:</span>q]))    <span class="co">#for sparse estimated eigenvectors</span>
<span class="co">#&gt; [1] 0.9971593 0.9969798 0.9924368</span></code></pre></div>
<p>Finally, the following plot shows the sparsity pattern of the eigenvectors (sparse computation vs. classical computation):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfcol =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(res_sparse1<span class="op">$</span>vectors[, <span class="dv">1</span>]<span class="op">*</span><span class="kw">sign</span>(res_sparse1<span class="op">$</span>vectors[<span class="dv">1</span>, <span class="dv">1</span>]), 
     <span class="dt">main =</span> <span class="st">&quot;First sparse eigenvector&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(V[, <span class="dv">1</span>]<span class="op">*</span><span class="kw">sign</span>(V[<span class="dv">1</span>, <span class="dv">1</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(res_sparse1<span class="op">$</span>vectors[, <span class="dv">2</span>]<span class="op">*</span><span class="kw">sign</span>(res_sparse1<span class="op">$</span>vectors[sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">2</span>]), 
     <span class="dt">main =</span> <span class="st">&quot;Second sparse eigenvector&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(V[, <span class="dv">2</span>]<span class="op">*</span><span class="kw">sign</span>(V[sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">2</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(res_sparse1<span class="op">$</span>vectors[, <span class="dv">3</span>]<span class="op">*</span><span class="kw">sign</span>(res_sparse1<span class="op">$</span>vectors[<span class="dv">2</span><span class="op">*</span>sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">3</span>]), 
     <span class="dt">main =</span> <span class="st">&quot;Third sparse eigenvector&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(V[, <span class="dv">3</span>]<span class="op">*</span><span class="kw">sign</span>(V[<span class="dv">2</span><span class="op">*</span>sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">3</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)

<span class="kw">plot</span>(res_standard<span class="op">$</span>vectors[, <span class="dv">1</span>]<span class="op">*</span><span class="kw">sign</span>(res_standard<span class="op">$</span>vectors[<span class="dv">1</span>, <span class="dv">1</span>]), 
     <span class="dt">main =</span> <span class="st">&quot;First regular eigenvector&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(V[, <span class="dv">1</span>]<span class="op">*</span><span class="kw">sign</span>(V[<span class="dv">1</span>, <span class="dv">1</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(res_standard<span class="op">$</span>vectors[, <span class="dv">2</span>]<span class="op">*</span><span class="kw">sign</span>(res_standard<span class="op">$</span>vectors[sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">2</span>]), 
     <span class="dt">main =</span> <span class="st">&quot;Second regular eigenvector&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(V[, <span class="dv">2</span>]<span class="op">*</span><span class="kw">sign</span>(V[sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">2</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(res_standard<span class="op">$</span>vectors[, <span class="dv">3</span>]<span class="op">*</span><span class="kw">sign</span>(res_standard<span class="op">$</span>vectors[<span class="dv">2</span><span class="op">*</span>sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">3</span>]), 
     <span class="dt">main =</span> <span class="st">&quot;Third regular eigenvector&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(V[, <span class="dv">3</span>]<span class="op">*</span><span class="kw">sign</span>(V[<span class="dv">2</span><span class="op">*</span>sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">3</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
</div>
<div id="covariance-matrix-estimation-with-sparse-eigenvectors" class="section level2">
<h2><span class="header-section-number">2.2</span> Covariance matrix estimation with sparse eigenvectors</h2>
<p>The function <code>spEigenCov()</code> requires more samples than the dimension (otherwise some regularization is required). Therefore, we generate data as previously with the only difference that we set the number of samples to be <code>n=600</code>.</p>
<p>Then, we compute the covariance matrix through the joint estimation of sparse eigenvectors and eigenvalues:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># computation of covariance matrix</span>
res_sparse3 &lt;-<span class="st"> </span><span class="kw">spEigenCov</span>(<span class="kw">cov</span>(X), q, rho)</code></pre></div>
<p>Again, we can assess how good the estimated eigenvectors are by computing the inner product with the original eigenvectors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show inner product between estimated eigenvectors and originals</span>
<span class="kw">abs</span>(<span class="kw">diag</span>(<span class="kw">t</span>(res_sparse3<span class="op">$</span>vectors[, <span class="dv">1</span><span class="op">:</span>q]) <span class="op">%*%</span><span class="st"> </span>V[, <span class="dv">1</span><span class="op">:</span>q]))    <span class="co">#for sparse estimated eigenvectors</span>
<span class="co">#&gt; [1] 0.9994578 0.9990208 0.9985083</span></code></pre></div>
<p>The following plot shows the sparsity pattern of the eigenvectors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfcol =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>))
<span class="kw">plot</span>(res_sparse3<span class="op">$</span>vectors[, <span class="dv">1</span>]<span class="op">*</span><span class="kw">sign</span>(res_sparse3<span class="op">$</span>vectors[<span class="dv">1</span>, <span class="dv">1</span>]), 
     <span class="dt">main =</span> <span class="st">&quot;First sparse eigenvector&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(V[, <span class="dv">1</span>]<span class="op">*</span><span class="kw">sign</span>(V[<span class="dv">1</span>, <span class="dv">1</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(res_sparse3<span class="op">$</span>vectors[, <span class="dv">2</span>]<span class="op">*</span><span class="kw">sign</span>(res_sparse3<span class="op">$</span>vectors[sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">2</span>]), 
     <span class="dt">main =</span> <span class="st">&quot;Second sparse eigenvector&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(V[, <span class="dv">2</span>]<span class="op">*</span><span class="kw">sign</span>(V[sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">2</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(res_sparse3<span class="op">$</span>vectors[, <span class="dv">3</span>]<span class="op">*</span><span class="kw">sign</span>(res_sparse3<span class="op">$</span>vectors[<span class="dv">2</span><span class="op">*</span>sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">3</span>]), 
     <span class="dt">main =</span> <span class="st">&quot;Third sparse eigenvector&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(V[, <span class="dv">3</span>]<span class="op">*</span><span class="kw">sign</span>(V[<span class="dv">2</span><span class="op">*</span>sp_card<span class="op">+</span><span class="dv">1</span>, <span class="dv">3</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAeAAAAJACAMAAABi/kXUAAAAilBMVEUAAAAAADoAAGYAOjoAOmYAOpAAZpAAZrY6AAA6ADo6AGY6OgA6OpA6ZmY6ZrY6kNtmAABmADpmAGZmOgBmZjpmtv+QOgCQOjqQOmaQZgCQkDqQtpCQ2/+2ZgC2Zjq2/7a2/9u2///bkDrb25Db/7bb/9vb////AAD/tmb/25D/29v//7b//9v///8tKxoVAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAUZElEQVR4nO2dDWPathaGlbQZpOsGWwfdB94Wb5fgBf3/v3d1JAvMR2OQDsZ+/T5tAljWsfDDkRXFsY0l0Jh7N4DcFgoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGJzBCX6bG2GxXT6+aMXUjNU3Bij4aS2Px1KqH9NjJgjO2VynDFbwEdV00mUrOt5cBoMVLFn3Nv8wffx7aczT/9w3n4VbebV+m3/84rpxEWHMw8ruV1z7Pj5+RurnPoPdqh+/uMd6mdvObxKi8Ismu2oScbL1mwvfQ/DedvEDF+zklWbh/seUCq+cDXeQflgFMyJ8t2J4GWLE5yHW03pjwmOsEpYt7GZfTSJuzEw29zYXx3UressABcsYay/FeRO1e8ETW38KRI0NWdZY0dlxawUl8bnEkufxUZbtxU8ki+NiHzRszi9y375x0OgJAxTczGDfv8qgOgoOr2rBbt/L18GKpQmjcFk5PpdYLrlteAzLomBbPP07n+1WLfeC/VMK1uZEsJW9PGmMetyrmMGSb80+2RX5vNutGZ6fZHC9nXBsfv7jebVblRl8a04EF5K+M+lJhfgqpKUT6hLtaMVanN1LPD4Gx9d+PZf4YczVOFTPwsF9X/OOO6SF4QsO42anzx9X4yjaDYHldeGOwL+HA2pcUQ7icdBbP/ciN8Z8t6xH0f4hfhBKn7yxmh9FW7+53SiagrsmcZ8jzmhRcE01DSNmNCg4IuNkvAQGFUx2UDA4FAwOBYNDweBQMDgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOBYOTItiQvqEsOKEOuSUUDE5Xgmn+TnQp2FBz93Qn2NSCzd6zOahgjp6Yo0eSws0FR0v7f2FoF1L6UHCtvn4SPgmGgrO4rWCz65eNORFc+9v13HWhNQ3BJgqm5ETUBJ/9qetVj4T3RuztM9iY3eSHPfkeM9Y01jr4Gb3+5gJRcCI3FmxOBJ8YvESwoeBU7iv4Qig4ndsLPszHVCg4EQoGp0PBWVBwIhQMDgWDQ8HgUDA4FAwOBYNz2182UPDdYQaDQ8HgUDA4yYKr59Xm9PpvFNw3UgVvv66qz2srX+/UoeC7kyr47ecXuX2Me3ivDgXfneQu+s+VS+KTewRRcN9IFuxvc2COr9xKwX2Do2hwkgVv/H4/vi0QBfeN5FH0MtxihKPonpMxim48fKsOBd+d3Az+FAXXIo5CUPC9SZ/Jmsp+5zG473AUDU6mYB6D+w4zGBwKBocTHeAMZaKDhhMZykQHBSeiNtFxtg4F352hTHRQcCJDGUVTcCIUDM5Q/rKBghNhBoNDweBQMDgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOf9kADjMYHAoGh4LBGcp50RScCM+LBofnRYPD86LB4XnR4HAUDQ5H0eBwFA0OR9HgDOUySkbxXtPg6Ai+bBRN7s5tR9Hk7mhfRon0jTzBLegltGLXwEbpbZP7svtQWoLPT3QkbyyvMXcKNfBGpUx0JG8sqzH3CjXwRqVMdCRvLKsx9wo18Eal/LoweWNZjblXqIE3KmWigwwITlqAQ8HgUDA4FAwOBYNDweBQMDgUDE624I0xi+xW+MnQECkvntwPd6ITqmjEyH6T5UytUY8vV0XKFfz206plJvOSIHPX6hApM145kflVjVCbiXzsVBolM4Izlfcnt+u210XKFVx9XoetZrD99S+XwSGSQrxyphRK7nCvEmn79ZeZyvsLRq+KlCvYfdBtkd1HSxcdIuXHc7F0QpWuB1SKNHNdtEaozYepOwJdFQlOsOu51D4rMUZmJJdrSoLtf2tbXteoPnTRQbBOb1g9r67sw95BqbMv5VdyWseNaxvVi0GWF6wyngl1lQZZLkGUBlkyitZqVHldo8B+TCp8smiFUvrZxqr+mHRlJE50gEPB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg0PB4FAwOBQMDgWDMzbBjQvK1BeoAGdsghtQMCLV539+mMtft5Tm45eFnNc0scXTeruc3btlN2KEgj+9yJ8sfHqppgvpsIuFLWbF5N4NuxUjFOzPJ5bTHIvFJpyEuV0+vXuptyEzcsEhcasp7pWixipYzhyfL6rnlRtquf8b2BQeq2A3yHr4Pgyytkv5IwbUg/DYBI8OCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgZnMILf5sY8XnRn5e3ysvX0K/eRoQiWHf82v+hyZV0Lrn5M3loHDEXw2/ziK5V1nITVtNfXUBuO4NqadNWSyNVUrgTsbPqe2yX3b/6O527xd7VgKXpav80/fokl5mHl1vwwffzbF+1iNeL6T4db9eMXH7VeLQQv/KLJYRNCA3bNcMH71cUPRbBcdFB2qnTT4WviFs3Ee+i9H1axqKr3cWkW7r+zsdgupXRSV35Y7YpCBVk5Po+Hgo0Jj7FKWCbXPlwcNUEyuNmM++6mEwYj2NpCUtbtUafuYSU729rw0osObjb+4sBRsPSd3qBfO2SZfx2KYizbeN4MEpftxU8ki4+aIIKbzbjX7vkGAxJsJRFL45EElEX+oSFYXm8bXXTIt7BGyHf/OhTFWCFQeN4MEpfF4LZ4+nc+s0dNEMHNZtxv/5xlSIJlP/pcsdZekMG+eBIzWCo0+2RXFGPtAllrTzLYhk4gHJuf/3he2aMmMINVcD52Pyrtj5ONY/Dh4VOqFGZRTWfhGCy2JNHqDI5FtTi7l3h8DG4Gl8QPY67DJjSPwRScinSMcriMEx4no+h1HAAfj6LdEFgqukP44+/hgFoXHUye1M99EDeg80HqZfsPQumT96gJhZFD927NO+6jcwxGcCqJ+xxmRouCT5Gfs66YWOk3FHwGORyAJDC+4LFDweBQMDgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDE6KYEP6hrLghDrkllAwOFCC+fE6hYLBGaLgb27EtK8yOoYk2DQezblttQkeo/ZBCjYHgs9pNf7fcVUKvrRItc7lwU3YRENw/Go+q1cKS0yzXebmTewfgxFsYuYeCTY77zudftWdYFN/izlNwRcUqda5OLKJroK9WrDZCY7L64XRrTGxSv0BGRVqglsnxjJ51eNmbewjg8ng18MJ1uMn+wxultiDtepCCm4vUq1zGa2CzZHg09Up+OIi1TqX8XpO3HnpLUUU3F6kWucy2gVfCgW3F6nWuQwKToOCwaFgcCgYHAoGh4LBoWBwhjMXTcFJMIPBoWBwKBicZMHV82pzetlGCu4bqYK3X1fV57WVr4vrZEHBaaQKfvv5Re765B4ur5MFBaeR3EX/uXJJfHJrLwruG8mC/d1JzPEFlym4b3AUDU6y4I3fWcd386LgvpE8il6GOwNxFN1zMkbRjYfL6mRBwWnkZvCnKLjee2oNO4aC00ifyZrKzuIxuO9wFA1OpmAeg/sOMxgcCgZnKBMden4p+IKi7ic6KDiRoUx0UHAiahMdF9TJgYITGcpEBwUnMpRRNAUnMpQT3yk4EWYwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg0PB4PCXDeAwg8GhYHB4ViU4PKsSHJ5VCQ7PqgSHZ1WCw1E0OBxFg8NRNDgcRYNz24uwKN4yVFEwODqCO78IC0ljKKNokoj2RVhI38gT3IJeQit2DWyU3ja5L7sPpSX4/ERH8sbyGnOnUANvVMpER/LGshpzr1ADb1TKREfyxrIac69QA29Uyq8LkzeW1Zh7hRp4o1ImOsiA4KQFOBQMDgWDQ8HgUDA4FAwOBYNDweBkC94Ys8huhZ8MDZHy4sndNCc6oYpGjOw3Wc7UGvX4clWkXMFvP61aZjIvCTJ3rQ6RMuOVE5lf1Qi1mcjHTqVRMiM4U3l/crNfe12kXMHV53XYagbbX/9yGRwiKcQrZ0qh5P7YKpG2X3+Zqby/YPSqSLmC3QfdFtl9tHTRIVJ+PBdLJ1TpekClSDPXRWuE2nyYuiPQVZHgBLueS+2zEmNkRnK5piTY/re25XWN6kMXHQTr9IbV8+rKPuwdlDr7Un4lp3XcuLZRvRhkecEq45lQV2mQ5RJEaZAlo2itRpXXNQrsx6TCJ4tWKKWfbazqj0lXRuJEBzgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg0PB4FAwOGMT3LigTH2BCnDGJrgBBSNSff7nh7n8dUtpPn5ZyHlNE1s8rbfL2b1bdiNGKPjTi/zJwqeXarqQDrtY2GJWTO7dsFsxQsH+fGI5zbFYbMJJmNvl07uXehsyIxccErea4l4paqyC5czx+aJ6Xrmhlvu/gU3hsQp2g6yH78Mga7uUP2JAPQiPTfDooGBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwhiXY3x7YmMnbPFzXart83N8i+eDFtWRV7jPDEmzrqw72QnD1Y/LWOmTYgo+KOk3CajqIa6cNVfCHqdzgXJzK88eXamq+qwVvl8Y8rd/mH7/4e6C7EvOw8lUe//ZF7rl/8NTP/afDrfrxi3usl7mP0W8SovCLJrtqEnEiW3l8Cd9D8H528UMV/LCSLA6Cvb6ndVXv49Is3H9nY7FdymoTX+pXi0XyMhiOz0Okp/XGhMdYJSyTax4u9osn7uVMMvhtLo7rNvSToQoOSqIW2eG7LrqU633bevEiZHS9WiiSlW0ZlMTnUjkGicv24ieSxXGxDxq6aL/IfTt7yOgHIILLur8Oq7iB9iJ6d5k8k0Tzr0NRGcbiXlN8LpVjkLgsbsUWT//OZ7tVy71g/5SCNbkgg63s80nMYMm3Zp/sinze7dac1XGPMtjut2Kr5z+eV7tVmcG35BuC4+FTVinMoprOwjFYbEmi1avFolqcPY0Uj8HxtV/PJX69jf1qs3Bwb7Shn4AItmdG0W4ILMfZwh2Bfw8H1LrID4djttfPvciNCUHqZfsPQumTN1bzo2gJLIfu3Zp33CnvMTTBF5O4z+FmtCh4j/ycJR0vFBTcQMbJYAmMK5gEKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsFJEWxI31AWnFCH3BIKBmfEgq9v65DeXYSCv/HyghqDYNSCTXiIL8+scbbGoKDg+O2MaHP4koKHKdjUX/X/+geLeumBdwoe1A44J9gcC979HBlW2b2//Yo9Z4SCza5vNk2NDcHGNJ/talHwkAR7vVFhPeVzKNi0Ch7Eu00qOrd228RYb3jV495v5QJGmMGvITdtI1PrD2cjb80+gw/X9SHqVSi4l7w2dR7+euRY8F7y4dx9/ZyCe8mrOaPxfcHNl3a/iIJ7yesZs+8ppeDMOl3TKvhi/RTcSzIFN6DgXkLB7UWqdbqGgtuLVOt0DQW3F6nW6RoKbi9SrdM1FNxedG7tQGZzuoCC24tU63QNBbcXqdbpGgpuL1Kt0zUU3F7kqJ5Xm9PLN1Jw30gVvP26qj6vrXxdXKcnUHB7kbVvP7/I3Z/cw+V1egIFtxc5/ly5JD65xRcF941kwf4uJeb4wssU3Dc4iqbg82z8Wzy+qxcF943kUfQy3CGIo+iekzGKbjxcVqcnUHB70T6DP0XB9XtWa9jtoOD2IhtuAcVjcP/hKJqC34PH4L7DDKZgpXC9gYLbiywnOtAFc6JjIIY50UHBZzmZ6LigTk+g4PYiy4kOeMF6dbqGgtuLzq0dyGxOF1Bwe5Fqna6h4PYi1TpdQ8HtRap1uoaC24tU63QNBbcXqdbpGgpuL1Kt0zUU3F6kWqdrKLi9SLVO11Bwe5Fqna6h4PYi1TpdQ8HtRap1uoaC24tU63QNBbcXnVs7kNmcLqDg9iLVOl1Dwe1FqnW6hoLbiyzPqkQXzLMqwQXzrEpwwTyrElwwz6pEF6xXp2souL3IchSNLpijaHDBHEWDC77sIiyKN/pURFFwH9ERPOBR9LgY4Sh6XGhfhIX0jTzBLegltGLXwEbpbZP7svtQWoLPT3QkbyyvMXcKNfBGpUx0JG8sqzH3CjXwRqVMdCRvLKsx9wo18Eal/LoweWNZjblXqIE3KmWigwwITlqAQ8HgUDA4FAwOBYNDweBQMDgUDE624I0xi+xW+MnQECkvntxNc6ITqmjEyH6T5UytUY8vV0XKFfz206plJvOSIHPX6hApM145kflVjVCbiXzsVBolM4IzlfcnN/u110XKFVx9XoetZrD99S+XwSGSQrxyphRK7o+tEmn79ZeZyvsLRq+KlCvYfdBtkd1HSxcdIuXHc7F0QpWuB1SKNHNdtEaozYepOwJdFQlOsOu51D4rMUZmJJdrSoLtf2tbXteoPnTRQbBOb1g9r67sw95BqbMv5VdyWseNaxvVi0GWF6wyngl1lQZZLkGUBlkyitZqVHldo8B+TCp8smiFUvrZxqr+mHRlJE50gEPB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg0PB4FAwOBQMDgWDMzbBjQvK1BeoAGdsghtQMCLV539+mMtft5Tm45eFnNc0scXTeruc3btlN2KEgj+9yJ8sfHqppgvpsIuFLWbF5N4NuxUjFOzPJ5bTHIvFJpyEuV0+vXuptyEzcsEhcasp7pWixipYzhyfL6rnlRtquf8b2BQeq2A3yHr4Pgyytkv5IwbUg/DYBI8OCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHD+D0Bl5e9kzaZ2AAAAAElFTkSuQmCC" /><!-- --></p>
<p>Finally, we can compute the error of the estimated covariance matrix (sparse eigenvector computation vs. classical computation):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show error between estimated and true covariance </span>
<span class="kw">norm</span>(<span class="kw">cov</span>(X) <span class="op">-</span><span class="st"> </span>R, <span class="dt">type =</span> <span class="st">'F'</span>) <span class="co">#for sample covariance matrix</span>
<span class="co">#&gt; [1] 48.42514</span>
<span class="kw">norm</span>(res_sparse3<span class="op">$</span>cov <span class="op">-</span><span class="st"> </span>R, <span class="dt">type =</span> <span class="st">'F'</span>) <span class="co">#for covariance with sparse eigenvectors</span>
<span class="co">#&gt; [1] 29.55455</span></code></pre></div>
</div>
<div id="complex-valued-inputs" class="section level2">
<h2><span class="header-section-number">2.3</span> Complex-valued inputs</h2>
<p>The previous examples illustrate the usage of the functions <code>spEigen()</code> and <code>spEigenCov()</code> for real-valued data and covariance matrices. However, both functions can handle complex-valued inputs.</p>
<p>Following the previous data generation procedure, we generate sparse complex eigenvectors by introducing a random phase in each entry:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="dv">500</span> <span class="co"># dimension</span>
n &lt;-<span class="st"> </span><span class="dv">600</span> <span class="co"># number of samples</span>
q &lt;-<span class="st"> </span><span class="dv">3</span> <span class="co"># number of sparse eigenvectors to be estimated</span>
sp_card &lt;-<span class="st"> </span><span class="fl">0.2</span><span class="op">*</span>m <span class="co"># cardinality of the sparse eigenvectors</span>
rho &lt;-<span class="st"> </span><span class="fl">0.5</span>

<span class="co"># generate non-overlapping sparse eigenvectors</span>
V &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, m<span class="op">*</span>q), <span class="dt">ncol =</span> q)
V[<span class="kw">cbind</span>(<span class="kw">seq</span>(<span class="dv">1</span>, q<span class="op">*</span>sp_card), <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>q, <span class="dt">each =</span> sp_card))] &lt;-<span class="st"> </span><span class="kw">exp</span>(1i<span class="op">*</span><span class="kw">runif</span>(sp_card<span class="op">*</span>q, <span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>pi))<span class="op">/</span><span class="kw">sqrt</span>(sp_card)
V &lt;-<span class="st"> </span><span class="kw">cbind</span>(V, <span class="kw">matrix</span>(<span class="kw">rnorm</span>(m<span class="op">*</span>(m<span class="op">-</span>q))<span class="op">*</span><span class="kw">exp</span>(1i<span class="op">*</span><span class="kw">runif</span>(m<span class="op">*</span>(m<span class="op">-</span>q),<span class="dv">0</span>,<span class="dv">2</span><span class="op">*</span>pi)), m, m<span class="op">-</span>q))

<span class="co"># keep first q eigenvectors the same (already orthogonal) and orthogonalize the rest</span>
<span class="cf">for</span> (i <span class="cf">in</span> (q<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>m) {
  tmp &lt;-<span class="st"> </span>V[,i]
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(i<span class="op">-</span><span class="dv">1</span>)) {
    tmp &lt;-<span class="st"> </span>tmp <span class="op">-</span><span class="st"> </span>(V[,i] <span class="op">%*%</span><span class="st"> </span><span class="kw">Conj</span>(V[,j])) <span class="op">%*%</span><span class="st"> </span>V[,j]
  }
  V[,i] &lt;-<span class="st"> </span>tmp<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">sum</span>(<span class="kw">abs</span>(tmp)<span class="op">^</span><span class="dv">2</span>))
}

<span class="co"># generate eigenvalues</span>
lmd &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span><span class="op">*</span><span class="kw">seq</span>(<span class="dt">from =</span> q, <span class="dt">to =</span> <span class="dv">1</span>), <span class="kw">rep</span>(<span class="dv">1</span>, m<span class="op">-</span>q))

<span class="co"># generate covariance matrix from sparse eigenvectors and eigenvalues</span>
R &lt;-<span class="st"> </span>V <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(lmd) <span class="op">%*%</span><span class="st"> </span><span class="kw">Conj</span>(<span class="kw">t</span>(V))

<span class="co"># generate data matrix from a zero-mean multivariate Gaussian distribution </span>
<span class="co"># with the constructed covariance</span>
X &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(n, <span class="kw">rep</span>(<span class="dv">0</span>, m), R)  <span class="co"># random data with underlying sparse structure</span></code></pre></div>
<p>Then, we compute the sparse complex eigenvectors (<code>spEigen()</code>) and the covariance matrix (<code>spEigenCov()</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># computation of sparse eigenvectors and covariance matrix</span>
S &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(n<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">Conj</span>(<span class="kw">t</span>(X)) <span class="op">%*%</span><span class="st"> </span>X
res_sparse4 &lt;-<span class="st"> </span><span class="kw">spEigen</span>(S, q, rho)
res_sparse5 &lt;-<span class="st"> </span><span class="kw">spEigenCov</span>(S, q, rho)</code></pre></div>
<p>The following plot shows the sparsity pattern of the eigenvectors for both functions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfcol =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(<span class="kw">abs</span>(res_sparse4<span class="op">$</span>vectors[, <span class="dv">1</span>]), <span class="dt">main =</span> <span class="st">&quot;First Sparse Eigenvector&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(<span class="kw">abs</span>(V[, <span class="dv">1</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">abs</span>(res_sparse4<span class="op">$</span>vectors[, <span class="dv">2</span>]), <span class="dt">main =</span> <span class="st">&quot;Second Sparse Eigenvector&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(<span class="kw">abs</span>(V[, <span class="dv">2</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">abs</span>(res_sparse4<span class="op">$</span>vectors[, <span class="dv">3</span>]), <span class="dt">main =</span> <span class="st">&quot;Third Sparse Eigenvector&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(<span class="kw">abs</span>(V[, <span class="dv">3</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)

<span class="kw">plot</span>(<span class="kw">abs</span>(res_sparse5<span class="op">$</span>vectors[, <span class="dv">1</span>]), <span class="dt">main =</span> <span class="st">&quot;First Sparse Eigenvector&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(<span class="kw">abs</span>(V[, <span class="dv">1</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">abs</span>(res_sparse5<span class="op">$</span>vectors[, <span class="dv">2</span>]), <span class="dt">main =</span> <span class="st">&quot;Second Sparse Eigenvector&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(<span class="kw">abs</span>(V[, <span class="dv">2</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">abs</span>(res_sparse5<span class="op">$</span>vectors[, <span class="dv">3</span>]), <span class="dt">main =</span> <span class="st">&quot;Third Sparse Eigenvector&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Index&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)
<span class="kw">lines</span>(<span class="kw">abs</span>(V[, <span class="dv">3</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>Finally, we can compute the error of the estimated covariance matrix (sparse eigenvector computation vs. classical computation):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show error between estimated and true covariance </span>
<span class="kw">norm</span>(S <span class="op">-</span><span class="st"> </span>R, <span class="dt">type =</span> <span class="st">'F'</span>) <span class="co">#for sample covariance matrix</span>
<span class="co">#&gt; Warning in norm(S - R, type = &quot;F&quot;): imaginary parts discarded in coercion</span>
<span class="co">#&gt; [1] 35.82586</span>
<span class="kw">norm</span>(res_sparse5<span class="op">$</span>cov <span class="op">-</span><span class="st"> </span>R, <span class="dt">type =</span> <span class="st">'F'</span>) <span class="co">#for covariance with sparse eigenvectors</span>
<span class="co">#&gt; Warning in norm(res_sparse5$cov - R, type = &quot;F&quot;): imaginary parts discarded</span>
<span class="co">#&gt; in coercion</span>
<span class="co">#&gt; [1] 20.01634</span></code></pre></div>
</div>
</div>
<div id="explanation-of-the-algorithms" class="section level1">
<h1><span class="header-section-number">3</span> Explanation of the algorithms</h1>
<div id="speigen-sparse-eigenvectors-from-a-given-covariance-matrix" class="section level2">
<h2><span class="header-section-number">3.1</span> <code>spEigen()</code>: Sparse eigenvectors from a given covariance matrix</h2>
<p>The goal of <code>spEigen()</code> is the estimation of the <span class="math inline">\(q\)</span> leading sparse eigenvectors (with <span class="math inline">\(q \leq \text{rank}(\mathbf{S})\)</span>) from an <span class="math inline">\(m\times m\)</span> covariance matrix <span class="math inline">\(\mathbf{S}\)</span> (typically the sample covariance matrix obtained from <span class="math inline">\(n\)</span> samples) based on <span class="citation">[1]</span>. The underlying optimization problem that is solved is <span class="math display">\[\begin{aligned}
      &amp;\underset{\mathbf{U}}{\text{maximize}}\quad \text{Tr} \left(\mathbf{U}^\top \mathbf{S} \mathbf{U} \text{Diag}   (\mathbf{d})\right) - \sum_{i=1}^{q}\rho_i\|\mathbf{u}_i\|_0\\
    &amp;\text{subject to}\quad \mathbf{U}^\top\mathbf{U}=\mathbf{I}_q,
  \end{aligned}\]</span> where <span class="math inline">\(\mathbf{U}\in\mathbb{R}^{m\times q}\)</span> is a matrix containing the <span class="math inline">\(q\)</span> leading eigenvectors, <span class="math inline">\(\mathbf{d}\)</span> is a vector of weights to ensure that <span class="math inline">\(\mathbf{U}\)</span> contains the leading eigenvectors without an arbitrary rotation, and the <span class="math inline">\(\rho_i\)</span>’s are the regularization parameters to control how much sparsity is desired. This problem is the typical PCA formulation with an extra penalty term in the objective that penalizes the cardinality of the eigenvectors, controled by the regularization parameters <span class="math inline">\(\rho_i\)</span>’s.</p>
<p>The <span class="math inline">\(\ell_0\)</span>-“norm” is approximated by the continuous and differentiable function <span class="math display">\[g_p^{\epsilon}\left(x \right)= \begin{cases}
    \frac{x^2}{2\epsilon(p+\epsilon)\log(1+1/p)},&amp; |x|\leq\epsilon,\\
    \frac{\log\left(\frac{p+|x|}{p+\epsilon}\right)+\frac{\epsilon}{2(p+\epsilon)}}{\log(1+1/p)},&amp; |x|&gt;\epsilon,
    \end{cases}\]</span> where <span class="math inline">\(p&gt;0\)</span> and <span class="math inline">\(0&lt;\epsilon\ll1\)</span> are parameters that control the approximation. This leads to the following approximate problem: <span class="math display">\[\begin{aligned}
      &amp;\underset{\mathbf{U}}{\text{maximize}}\quad \text{Tr} \left(\mathbf{U}^\top \mathbf{S} \mathbf{U} \text{Diag}   (\mathbf{d})\right) - \sum_{j=1}^{q}\rho_j\sum_{i=1}^{m}g_p^{\epsilon}\left(u_{ij}\right)\\
    &amp;\text{subject to}\quad \mathbf{U}^\top\mathbf{U}=\mathbf{I}_q.
  \end{aligned}\]</span></p>
<p>This problem can be solved via Majorization-Minimization (MM) <span class="citation">[2]</span> with an iterative closed-form update algorithm. For this, at each iteration (denoted by <span class="math inline">\(k\)</span>) two key quantities are needed:</p>
<p><span class="math display">\[\mathbf{G}^{(k)} = \mathbf{S}\mathbf{U}^{(k)}\text{Diag}(\mathbf{d})\]</span><br />
<span class="math display">\[\mathbf{H}^{(k)}=\left[\text{diag}\left(\mathbf{w}^{(k)}-\mathbf{w}_{\max}^{(k)}\otimes\mathbf{1}_{m}\right)\mathbf{\tilde{u}}^{(k)}\right]_{m\times q},\]</span> where <span class="math display">\[w_{i}^{(k)}= \begin{cases}
        \frac{\rho_i}{2\epsilon(p+\epsilon)\log(1+1/p)},&amp; |\tilde{u}^{(k)}_{i}|\leq\epsilon,\\
        \frac{\rho_i}{2\log(1+1/p)|\tilde{u}^{(k)}_{i}|\left(|\tilde{u}^{(k)}_{i}|+p\right)},&amp;                |\tilde{u}^{(k)}_{i}|&gt;\epsilon,
        \end{cases}\]</span> with <span class="math inline">\(\mathbf{w}\in\mathbb{R}_+^{mq}\)</span>, <span class="math inline">\(\mathbf{\tilde{u}}^{(k)} = \text{vec}(\mathbf{U}^{(k)})\in\mathbb{R}_+^{mq}\)</span>, <span class="math inline">\(\mathbf{w}_{\max}\in\mathbb{R}^q_+\)</span>, with <span class="math inline">\(w_{\max,i}\)</span> being the maximum weight that corresponds to the <span class="math inline">\(i\)</span>-th eigenvector <span class="math inline">\(\mathbf{u}^{(k)}_{i}\)</span>.</p>
<p>The iterative closed-form update algorithm is:</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(k=0\)</span> and choose an initial point <span class="math inline">\(\mathbf{U}^{(0)}\)</span><br />
</li>
<li>Compute <span class="math inline">\(\mathbf{G}^{(k)}\)</span> and <span class="math inline">\(\mathbf{H}^{(k)}\)</span><br />
</li>
<li>Compute <span class="math inline">\(\mathbf{V}_{\text{L}}\)</span>, <span class="math inline">\(\mathbf{V}_{\text{R}}\)</span> as the left and right singular vectors of <span class="math inline">\(\left(\mathbf{G}^{(k)} - \mathbf{H}^{(k)}\right)\)</span><br />
</li>
<li><span class="math inline">\(\mathbf{U}^{(k+1)} \gets \mathbf{V}_{\text{L}}\mathbf{V}_{\text{R}}^\top\)</span><br />
</li>
<li><span class="math inline">\(k \gets k+1\)</span><br />
</li>
<li>Repeat steps 2-5 until convergence<br />
</li>
<li>Return <span class="math inline">\(\mathbf{U}^{(k)}\)</span></li>
</ol>
</blockquote>
<p>The initial point of the algorithm <span class="math inline">\(\mathbf{U}^{(0)}\)</span> is set by default to the <span class="math inline">\(q\)</span> leading standard eigenvectors, unless the user specifies otherwise. Internally, all the computations of <span class="math inline">\(\mathbf{G}^{(k)}\)</span> and <span class="math inline">\(\mathbf{H}^{(k)}\)</span> are done through the eigenvalue decomposition (EVD) of <span class="math inline">\(\mathbf{S}\)</span>. Since we can also retrieve the eigenvectors and eigenvalues of <span class="math inline">\(\mathbf{S}\)</span> through the singular value decomposition (SVD) of the data matrix <span class="math inline">\(\mathbf{X}\)</span>, with <span class="math inline">\(\mathbf{S} = \frac{1}{n-1}\mathbf{X}^\top\mathbf{X}\)</span>, it becomes possible to use as an input to <code>spEigen()</code> either the covariance matrix <span class="math inline">\(\mathbf{S}\)</span> or directly the data matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Although <span class="math inline">\(\mathbf{H}^{(k)}\)</span> does not depend directly on <span class="math inline">\(\mathbf{S}\)</span>, the parameters <span class="math inline">\(\rho_j\)</span> are set based on its eigenvalues. In particular, each <span class="math inline">\(\rho_j\)</span> takes a value in an interval <span class="math inline">\([0, \rho_j^{\text{max}}]\)</span> based on the input variable <span class="math inline">\(\rho\in[0, 1]\)</span> that the user selects, i.e., <span class="math inline">\(\rho_j = \rho\rho_j^{\text{max}}\)</span>. The uppperbound <span class="math inline">\(\rho_j^{\text{max}}\)</span> depends, among others, on the eigenvalues of <span class="math inline">\(\mathbf{S}\)</span>. Note that the theoretical upperbound is derived based on the initial problem and not the approximate. Therefore, although a suggested range for <span class="math inline">\(\rho\)</span> is the interval <span class="math inline">\([0, 1]\)</span>, any nonnegative value is accepted by the algorithm.</p>
<p>Finally, note that the approximate problem is controlled by the parameters <span class="math inline">\(p, \epsilon\)</span>, and in particular, as <span class="math inline">\(p\rightarrow0\)</span> we get <span class="math inline">\(\rho_p\rightarrow\ell_0\)</span>. However, by setting small values to <span class="math inline">\(p, \epsilon\)</span>, it is likely that the algorithm will get stuck to a local minimum. To solve this issue we start with large values for <span class="math inline">\(p, \epsilon\)</span>, i.e., a “loose” approximation, and solve the corresponding optimization problem. Then, we sequentially decrease <span class="math inline">\(p, \epsilon\)</span>, i.e., we “tighten” the approximation, and solve the problem again using the previous solution as an initial point. In practice we are interested only in the last, “tightest” problem. For each problem that is solved (i.e., for fixed <span class="math inline">\(p, \epsilon\)</span>) we utilize an acceleration scheme that increases the convergence speed of the MM algorithm. For details, please refer to <span class="citation">[3]</span>.</p>
</div>
<div id="speigencov-covariance-matrix-estimation-with-sparse-eigenvectors" class="section level2">
<h2><span class="header-section-number">3.2</span> <code>spEigenCov()</code>: Covariance matrix estimation with sparse eigenvectors</h2>
<p>The function <code>spEigenCov()</code> estimates a covariance matrix through the joint estimation of its sparse (orthogonal) eigenvectors and eigenvalues <span class="citation">[1]</span>, i.e., <span class="math inline">\(\mathbf{\Sigma}=\mathbf{U}\mathbf{\Xi}\mathbf{U}^\top\)</span>, with <span class="math inline">\(\mathbf{U},\mathbf{\Xi}\in\mathbb{R}^{m\times m}\)</span> and <span class="math inline">\(\mathbf{\Xi}=\text{Diag}(\mathbf{\xi})\)</span>. The underlying optimization problem that is solved is the maximization of the log-likelihood under a Gaussian distribution for the data: <span class="math display">\[\begin{aligned}
    &amp;\underset{\mathbf{U},\mathbf{\Xi}}{\text{minimize}}\quad\log\det\left(\mathbf{\Xi}\right)+\text{Tr}\left(\mathbf{S}\mathbf{U}\mathbf{\Xi}^{-1}\mathbf{U}^\top\right) + \sum_{i=1}^{m}\rho_i\|\mathbf{u}_i\|_0\\
        &amp;\!\begin{array}{lll}
        \text{subject to} &amp; \mathbf{\Xi}\succcurlyeq0, &amp;\\
         &amp; \xi_{i}\geq \xi_{i+1}, &amp; i=1,\dots, q-1,\\
         &amp; \xi_{q}\geq \xi_{q+i}, &amp; i=1,\dots,m-q,\\
         &amp; \mathbf{U}^\top\mathbf{U}=\mathbf{I}_m,
        \end{array}
    \end{aligned}\]</span> where <span class="math inline">\(\mathbf{S}\in\mathbb{R}^{m\times m}\)</span> is the sample covariance matrix and <span class="math inline">\(q\)</span> is the number of eigenvectors we impose sparsity on (i.e., <span class="math inline">\(\rho_i = 0\)</span> for <span class="math inline">\(i&gt;q\)</span>). The constraints ensure that the eigenvalues will be positive, while the first <span class="math inline">\(q\)</span> of them will be the largest and in descending order. This is important since in case of a swap of the eigenvalues during the estimation process we would impose sparsity on different eigenvectors at each iteration of the algorithm with disastrous consequences. Finally, the last constraint ensures the orthogonality of the eigenvectors.</p>
<p>Again, the <span class="math inline">\(\ell_0\)</span>-“norm” is approximated by the continuous and differentiable function <span class="math inline">\(g_p^{\epsilon}()\)</span> which leads to the following approximate problem: <span class="math display">\[\begin{aligned}
    &amp;\underset{\mathbf{U},\mathbf{\Xi}}{\text{minimize}}\quad\log\det\left(\mathbf{\Xi}\right)+\text{Tr}\left(\mathbf{S}\mathbf{U}\mathbf{\Xi}^{-1}\mathbf{U}^\top\right) +\sum_{j=1}^{m}\rho_j \sum_{i=1}^{m}g_p^{\epsilon}\left(u_{ij}\right)\\
        &amp;\!\begin{array}{lll}
        \text{subject to} &amp; \mathbf{\Xi}\succcurlyeq0, &amp;\\
         &amp; \xi_{i}\geq \xi_{i+1}, &amp; i=1,\dots, q-1,\\
         &amp; \xi_{q}\geq \xi_{q+i}, &amp; i=1,\dots,m-q,\\
         &amp; \mathbf{U}^\top\mathbf{U}=\mathbf{I}_m,
        \end{array}
    \end{aligned}\]</span></p>
<p>This problem can be solved via Majorization-Minimization (MM) <span class="citation">[2]</span> with an iterative semi-closed-form update algorithm. In particular, with a proper majorization, the eigenvector and eigenvalue estimation decouples. Therefore, at each iteration we need to solve the following two problems:</p>
<ul>
<li><p>Eigenvector optimization:</p>
<p><span class="math display">\[\begin{aligned}
    &amp;\underset{\mathbf{U}}{\text{minimize}}\quad \text{Tr}\left({\mathbf{H}^{(k)}}^\top\mathbf{U}\right)\\
    &amp;\text{subject to}\quad \mathbf{U}^\top\mathbf{U}=\mathbf{I}_m,
\end{aligned}\]</span> where <span class="math inline">\(\mathbf{H}^{(k)}=\left[\text{diag}\left(\mathbf{w}^{(k)}-\mathbf{w}_{\max}^{(k)}\otimes\mathbf{1}_{m}\right)\mathbf{\tilde{u}}^{(k)}\right]_{m\times m} + \left(\mathbf{S}-\lambda_{\max}^{(\mathbf{S})}\mathbf{I}_m\right) \mathbf{U}^{(k)} \left(\mathbf{\Xi}^{(k)}\right)^{-1}\)</span>. Again the vector <span class="math inline">\(\mathbf{w}\)</span> is given by <span class="math display">\[w_{i}^{(k)}= \begin{cases}
    \frac{\rho_i}{2\epsilon(p+\epsilon)\log(1+1/p)},&amp; |\tilde{u}^{(k)}_{i}|\leq\epsilon,\\
    \frac{\rho_i}{2\log(1+1/p)|\tilde{u}^{(k)}_{i}|\left(|\tilde{u}^{(k)}_{i}|+p\right)},&amp;                |\tilde{u}^{(k)}_{i}|&gt;\epsilon,
    \end{cases}\]</span> with <span class="math inline">\(\mathbf{w}\in\mathbb{R}_+^{m^2}\)</span>, <span class="math inline">\(\mathbf{\tilde{u}}^{(k)} = \text{vec}(\mathbf{U}^{(k)})\in\mathbb{R}_+^{m^2}\)</span>, <span class="math inline">\(\mathbf{w}_{\max}\in\mathbb{R}^m_+\)</span>, with <span class="math inline">\(w_{\max,i}\)</span> being the maximum weight that corresponds to the <span class="math inline">\(i\)</span>-th eigenvector <span class="math inline">\(\mathbf{u}^{(k)}_{i}\)</span>.</p>
<p>The optimal solution of this problem is <span class="math inline">\(\mathbf{U} = \mathbf{V}_L\mathbf{V}_R^\top\)</span> where <span class="math inline">\(\mathbf{V}_L, \mathbf{V}_R\)</span> are the lest and right singular vectors of <span class="math inline">\(\mathbf{H}^{(k)}\)</span>, respectively.</p></li>
<li><p>Eigenvalue optimization:</p>
<p><span class="math display">\[\begin{aligned}
    &amp;\underset{\mathbf{\xi}}{\text{minimize}}\quad \sum_{i=1}^{m}\left(\log\xi_i+\alpha_i^{(k)}\xi_i+\lambda_{\max}^{(\mathbf{S})}\frac{1}{\xi_i}\right)\\
    &amp;\!\begin{array}{lll}
    \text{subject to} &amp; \xi_{i}\geq \xi_{i+1}, &amp; i=1,\dots, q-1,\\
     &amp; \xi_{q}\geq \xi_{q+i}, &amp; i=1,\dots,m-q,
    \end{array}
\end{aligned}\]</span> where <span class="math inline">\(\mathbf{\alpha}^{(k)} = \text{diag}\left(\left(\mathbf{\Xi}^{(k)}\right)^{-1} {\mathbf{U}^{(k)}}^T \left(\mathbf{S}-\lambda_{\max}^{(\mathbf{S})}\mathbf{I}_m\right) \mathbf{U}^{(k)} \left(\mathbf{\Xi}^{(k)}\right)^{-1}\right)\)</span>. This problem is not convex. However, it can be transformed to a convex one by the variable transformation <span class="math inline">\(\mathbf{\phi} = 1/\mathbf{\xi}\)</span>. Solving the KKT equations of the transformed convex formulation we can derive a finite-step algorithm that gives the optimal solution of the problem.</p></li>
</ul>
<p>The overall iterative semi-closed-form update algorithm is:</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(k=0\)</span> and choose initial points <span class="math inline">\(\mathbf{U}^{(0)}\)</span>, <span class="math inline">\(\mathbf{\xi}^{(0)}\)</span><br />
</li>
<li>Compute <span class="math inline">\(\mathbf{H}^{(k)}\)</span><br />
</li>
<li>Compute <span class="math inline">\(\mathbf{V}_{\text{L}}\)</span>, <span class="math inline">\(\mathbf{V}_{\text{R}}\)</span> as the left and right singular vectors of <span class="math inline">\(\mathbf{H}^{(k)}\)</span><br />
</li>
<li><span class="math inline">\(\mathbf{U}^{(k+1)} \gets \mathbf{V}_{\text{L}}\mathbf{V}_{\text{R}}^\top\)</span><br />
</li>
<li>Compute <span class="math inline">\(\mathbf{\alpha}^{(k)}\)</span><br />
</li>
<li>Get <span class="math inline">\(\mathbf{\xi}^{(k+1)}\)</span> from the finite-step algorithm<br />
</li>
<li><span class="math inline">\(k \gets k+1\)</span><br />
</li>
<li>Repeat steps 2-7 until convergence<br />
</li>
<li>Return <span class="math inline">\(\mathbf{U}^{(k)}\)</span>, <span class="math inline">\(\mathbf{\xi}^{(k)}\)</span></li>
</ol>
</blockquote>
<p>As in the <code>spEigen()</code> function, we sequentially decrease the values of <span class="math inline">\(p, \epsilon\)</span>, and increase the convergence speed of each problem using the acceleration method proposed in <span class="citation">[3]</span>.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<p>   </p>
<div id="refs" class="references">
<div id="ref-BenSunBabuPal2017">
<p>[1] K. Benidis, Y. Sun, P. Babu, and D. P. Palomar, “Orthogonal sparce PCA and covariance estimation via Procrustes reformulation,” <em>IEEE Transactions on Signal Processing</em>, vol. 64, no. 23, pp. 6211–6226, Dec. 2016.</p>
</div>
<div id="ref-SunBabPal2018">
<p>[2] Y. Sun, P. Babu, and D. P. Palomar, “Majorization-minimization algorithms in signal processing, communications, and machine learning,” <em>IEEE Transactions on Signal Processing</em>, vol. 65, no. 3, pp. 794–816, Feb. 2017.</p>
</div>
<div id="ref-Varadhan2008">
<p>[3] R. Varadhan and C. Roland, “Simple and globally convergent methods for accelerating the convergence of any em algorithm,” <em>Scandinavian Journal of Statistics</em>, vol. 35, no. 2, pp. 335–353, 2008.</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
