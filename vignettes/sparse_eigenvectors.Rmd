---
title: "Computing sparse eigenvectors"
author: "Konstantinos Benidis and Daniel P. Palomar"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{amsmath}
output: 
  rmarkdown::html_vignette:
    number_sections: true
    toc: no
    toc_depth: 2
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Computing sparse eigenvectors}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
#See: https://github.com/arorar/covmat/blob/master/inst/doc/CovarianceEstimation.pdf
```

This vignette illustrates the computation of sparse eigenvectors with the package `sparseEigen`.

# Comparison with other packages
hulsdhiuvhsdal

# Usage of the package

## Computation of sparse eigenvectors of a given matrix

We start by loading the package and generating synthetic data with sparse eigenvectors:
```{r}
library(sparseEigen)
set.seed(42)

# parameters 
m <- 500  # dimension
n <- 100  # number of samples
q <- 3  # number of sparse eigenvectors to be estimated
sp_card <- 0.2*m  # cardinality of each sparse eigenvector
rho <- 0.6  # sparsity level

# generate non-overlapping sparse eigenvectors
V <- matrix(rnorm(m^2), ncol = m)
tmp <- matrix(0, m, q)
for (i in 1:max(q, 2)) {
  ind1 <- (i - 1)*sp_card + 1
  ind2 <- i*sp_card
  tmp[ind1:ind2, i] = 1/sqrt(sp_card)
  V[, i] <- tmp[, i]
}
V <- qr.Q(qr(V))  # keep first q eigenvectors the same (already orthogonal) and orthogonalize the rest

# generate eigenvalues
lmd <- rep(1, m)
lmd[1:q] <- 100*seq(from = q, to = 1)

# generate covariance matrix from sparse eigenvectors and eigenvalues
R <- V %*% diag(lmd) %*% t(V)

# generate data matrix from a zero-mean multivariate Gaussian distribution with the constructed covariance
X <- MASS::mvrnorm(n, rep(0, m), R)  # random data with underlying sparse structure
X <- scale(X, center = TRUE, scale = FALSE)  # center the data
```

Then we estimate the covariance matrix with `cov(X)` and compute its sparse eigenvectors:
```{r}
# computation of sparse eigenvectors
res_standard <- eigen(cov(X))
res_sparse <- spEigen(X, q, rho, data = TRUE)
# res_sparse <- spEigen(cov(X), q, rho)
```

We can assess how good the estimated eigenvectors are by computing the inner product with the original eigenvectors (the closer to 1 the better):
```{r}
# show inner product between estimated eigenvectors and originals
abs(diag(t(res_standard$vectors) %*% V[, 1:q]))  #for standard estimated eigenvectors
abs(diag(t(res_sparse$vectors) %*% V[, 1:q]))    #for sparse estimated eigenvectors
```


Finally, the following plot shows the sparsity pattern of the eigenvectors:
```{r, fig.height = 7, fig.width = 7}
par(mfcol = c(3, 2))
plot(res_sparse$vectors[, 1]*sign(res_sparse$vectors[1, 1]), main = "First Sparse Eigenvector", xlab = "Index", ylab = "", type = "h")
lines(V[, 1]*sign(V[1, 1]), col = "red")
plot(res_sparse$vectors[, 2]*sign(res_sparse$vectors[sp_card+1, 2]), main = "Second Sparse Eigenvector", xlab = "Index", ylab = "", type = "h")
lines(V[, 2]*sign(V[sp_card+1, 2]), col = "red")
plot(res_sparse$vectors[, 3]*sign(res_sparse$vectors[2*sp_card+1, 3]), main = "Third Sparse Eigenvector", xlab = "Index", ylab = "", type = "h")
lines(V[, 3]*sign(V[2*sp_card+1, 3]), col = "red")

plot(res_standard$vectors[, 1]*sign(res_standard$vectors[1, 1]), main = "First Eigenvector", xlab = "Index", ylab = "", type = "h")
lines(V[, 1]*sign(V[1, 1]), col = "red")
plot(res_standard$vectors[, 2]*sign(res_standard$vectors[sp_card+1, 2]), main = "Second Eigenvector", xlab = "Index", ylab = "", type = "h")
lines(V[, 2]*sign(V[sp_card+1, 2]), col = "red")
plot(res_standard$vectors[, 3]*sign(res_standard$vectors[2*sp_card+1, 3]), main = "Third Eigenvector", xlab = "Index", ylab = "", type = "h")
lines(V[, 3]*sign(V[2*sp_card+1, 3]), col = "red")
```

## Covariance matrix estimation with sparse eigenvectors



# Explanation of the algorithms

## *spEigen*: Sparse eigenvectors extraction

The goal of *spEigen* is the estimation of the $q$ leading sparse eigenvectors. The underlying optimization problem that is solved is:
$$\begin{equation}
	\begin{aligned}
	  &\underset{\mathbf{U}}{\text{maximize}}\quad \text{Tr} \left(\mathbf{U}^\top \mathbf{S} \mathbf{U} \text{Diag}   (\mathbf{d})\right) - \sum_{i=1}^{q}\rho_i\|\mathbf{u}_i\|_0\\
  	&\text{subject to}\quad \mathbf{U}^\top\mathbf{U}=\mathbf{I}_q,
  \end{aligned}
	\end{equation}$$ 
where $\mathbf{U}\in\mathbb{R}^{m\times q}$ is a matrix containing the $q$ leading eigenvectors (with $q \leq \text{rank}(\mathbf{S})$), $\mathbf{S}$ is the sample covariance matrix and $\mathbf{d}$ some weight parameters to ensure that the term $\text{Tr} \left(\mathbf{U}^\top \mathbf{S} \mathbf{U} \text{Diag}   (\mathbf{d})\right)$ will not be a constant. This problem is the typical PCA formulation with an extra penalty term in the objective that penalizes the cardinality of the eigenvectors, controled by the regularization parameters $\rho_j$.	 

The $\ell_0$-"norm" is approximated by the continuous and differentiable function
$$\begin{equation}
		g_p^{\epsilon}\left(x \right)= \begin{cases}
		\frac{x^2}{2\epsilon(p+\epsilon)\log(1+1/p)},& |x|\leq\epsilon,\\
		\frac{\log\left(\frac{p+|x|}{p+\epsilon}\right)+\frac{\epsilon}{2(p+\epsilon)}}{\log(1+1/p)},& |x|>\epsilon,
		\end{cases}
		\end{equation}$$
where $p>0$ and $0<\epsilon\ll1$ are parameters that control the approximation. This leads to the following approximate problem:
$$\begin{equation}
	\begin{aligned}
	  &\underset{\mathbf{U}}{\text{maximize}}\quad \text{Tr} \left(\mathbf{U}^\top \mathbf{S} \mathbf{U} \text{Diag}   (\mathbf{d})\right) - \sum_{j=1}^{q}\rho_j\sum_{i=1}^{m}g_p^{\epsilon}\left(u_{ij}\right)\\
  	&\text{subject to}\quad \mathbf{U}^\top\mathbf{U}=\mathbf{I}_q.
  \end{aligned}
	\end{equation}$$ 


This problem can be solved via MM with an iterative closed-form update algorithm. For this, at each iteration (denoted by $k$) two key quantities are needed:

* $$\mathbf{G}^{(k)} = \mathbf{S}\mathbf{U}^{(k)}\text{Diag}(\mathbf{d})$$  
* $$\mathbf{H}^{(k)}=\left[\text{diag}\left(\mathbf{w}^{(k)}-\mathbf{w}_{\max}^{(k)}\otimes\mathbf{1}_{m}\right)\mathbf{u}^{(k)}\right]_{m\times q},$$ 
where
$$\begin{equation*}
		w_{i}= \begin{cases}
		\frac{\rho_i}{2\epsilon(p+\epsilon)\log(1+1/p)},& |u^{(k)}_{i}|\leq\epsilon,\\
		\frac{\rho_i}{2\log(1+1/p)|u^{(k)}_{i}|\left(|u^{(k)}_{i}|+p\right)},&                |u^{(k)}_{i}|>\epsilon,
		\end{cases}
		\end{equation*}$$
with $\mathbf{w}\in\mathbb{R}_+^{mq}$, $\mathbf{u}^{(k)} = \text{vec}(\mathbf{U}^{(k)})$, $\mathbf{w}_{\max}\in\mathbb{R}^q_+$, with $w_{\max,i}$ being the maximum weight that corresponds to $\mathbf{u}^{(k)}_{i}$.

The iterative closed-form update algorithm is:  

> 1. Set $k=0$ and choose an initial point $\mathbf{U}^{(0)}$  
2. Compute $\mathbf{G}^{(k)}$ and $\mathbf{H}^{(k)}$  
3. Compute $\mathbf{V}_{\text{L}}$, $\mathbf{V}_{\text{R}}$, the left and right singular vectors of $\left(\mathbf{G}^{(k)} - \mathbf{H}^{(k)}\right)$  
4. $\mathbf{U}^{(k+1)} \gets \mathbf{V}_{\text{L}}\mathbf{V}_{\text{R}}^\top$  
5. $k \gets k+1$  
6. Repeat steps 2-5 until convergence  
7. Return $\mathbf{U}^{(k)}$  

Internally, all the computations of $\mathbf{G}^{(k)}$ and $\mathbf{H}^{(k)}$ are done through the EVD of $\mathbf{S}$. Since we can retrieve the eigenvectors and eigenvalues of $\mathbf{S}$ through the SVD of the data matrix $\mathbf{X}$, with $\mathbf{S} = \frac{1}{n-1}\mathbf{X}^\top\mathbf{X}$, it becomes possible to use as an input to *spEigen* either the covariance matrix $\mathbf{S}$ or directly the data matrix $\mathbf{X}$. Although $\mathbf{H}^{(k)}$ does not depend directly on $\mathbf{S}$, the parameters $\rho_j$ are set based on its eigenvalues.   



## *spEigenCov*: Covariance matrix estimation with sparse eigenvectors










